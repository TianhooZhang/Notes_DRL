# 多臂赌博机

**强化学习与其它机器学习方法最大的不同**:强化学习的训练信号是用评估给定动作的好坏的，而不是通过正确范例进行直接指导的。

- 评估性反馈：表明当前采取动作的好坏程度
- 指导性反馈：表明当前应采取的最好的动作

```我个人理解为评估性反馈是对过程的反馈，即基于某一个原则对每一个动作进行评价，而指导性反馈只是告诉你，你应该去做什么，而不说你当前做的与你应该做的之间的关系。举个例子，假设一道题是10分，你有其中一步做错了，评估性老师会告诉你当前得了7分，而指导性老师会告诉你，你得了0分。```

所以说，评估性反馈依赖于当前采取的动作，即采取不同的动作会得到不同的反馈；而指导性反馈则不依赖于当前采取的动作，即采取不同的动作也会得到相同的反馈。

本章节主要是通过讨论”K臂赌博机问题“，一个典型的非关联的评估性反馈问题来为之后的关联的完全强化学习问题做基础。

```非关联的：动作不会使环境发生改变。```

## 一个k臂赌博机问题

问题描述：重复在k个选项或动作中进行选择，每次选择会有一个收益，每个动作的收益服从一种分布。目标是在某一段时间内最大化总收益的期望。
我们称某种动作带来的收益的期望为这个动作的*价值*。
记 $t$ 时刻选择的动作为 $A_t$ ，收益为 $R_t$ ，任意动作 $a$ 的价值为 $q_*(a)$ ，即 $q_*(a) \dot= \mathbb{E}[R_t|A_t=a]$ 。
记动作 $a$ 在时刻 $t$ 的价值的估计为 $Q_t(a)$，我们期望 $Q_t(a)$ 接近 $q_*(a)$。

如果持续对动作的价值进行估计，那么在任一时刻都会至少有一个动作的估计价值是最高的，我们将这些对应最高估计价值的动作成为**贪心动作**。如果选择贪心动作，即称之为**开发**，如果不是基于贪心选择动作，而是随机选择动作，即为**试探**。
开发会最大化当前动作的收益，而试探则会带来总体收益的提升。
```这个很好理解，一个是守旧一个是创新，守旧会带来当前动作更为准确的价值估计，而创新则会提高其它动作的价值估计，从而准确得知当前动作相对于其它动作的价值估计。```
强化学习需要去解决的一个问题就是：**开发与试探的平衡**。
本文先不讨论如何去平衡开发与试探，而是通过实验验证：平衡优于贪心，也即*开发+试探>开发*。

## 动作-价值方法

使用价值估计来进行动作选择的方法称之为”动作-价值方法“。
由于动作价值的真实值是选择这个动作时的期望收益，因此最简单的就是通过计算实际收益的平均值来估计动作的价值。
$Q_t(a) \dot= \frac{t时刻前通过执行动作a得到的收益总和}{t时刻前执行动作a的次数}=\frac{\sum_{i=1}^{t-1}R_i·\mathbb{I}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb{I}_{A_i=a}}$
$\mathbb{I_{predicate}}$表示随机变量，当$predicate$为真时，其值为1，反之为0。
当分母为$0$时，设置$Q_t(a)$为某个常数，比如说$Q_t(a)=0$。
根据大数定律，$Q_t(a)$会收敛到$q_*(a)$。

可以根据**贪心原则**选择动作，即选择具有最高估计值的动作。
$A_t \dot= argmax_aQ_t(a)$
其中$argmax_a$ 是使得$Q_t(a)$的值最大的动作$a$。

还可以在贪心原则上加入探索，形成 **$\epsilon$-贪心** 策略。
即以$\epsilon$概率从所有动作中随机选择一个动作，以$1-\epsilon$的概率按照贪心来选择动作。
由此我们可以看出，在无限次选择动作时，每个动作都会被无限次的采样，从而满足所有动作的估计价值逼近其实际价值。最终你那个最优动作的选择概率会大于$1-\epsilon$。

## 10 臂测试平台


## 习题

### 练习2.1

$P_{贪心动作被选择}=0.5+0.5\times0.5=0.75$
```第一个0.5是贪心选择的概率，第二个0.5是试错时选择```

