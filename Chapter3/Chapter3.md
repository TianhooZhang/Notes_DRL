# 有限马尔科夫决策过程

马尔科夫过程（MDP）是序列决策的经典形式化表达，其动作不仅影响当前的即使收益，还影响后续的情境（又称状态）以及未来的收益。因此，MDP涉及了**延迟收益**，由此也就有了在当前收益和延迟收益之间权衡的需求。

MDP是强化学习问题在数学上的理想化形式。

## 3.1 ”智能体-环境“交互接口

MDP是一种通过交互学习来实现目标的理论框架。进行学习及实施决策的机器被称为**智能体**。智能体之外所有与其相互作用的事务都被称为**环境**。智能体选择动作，环境对这些动作做出相应的响应，并向智能体呈现出新的状态，并产生一个**收益**。收益是智能体动作选择过程中想要最大化的目标。
$$
S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,\dots
$$

在有限MDP中，状态S，动作A，和收益R的集合（S、A和R）都只有有限个元素。在这种情况下，随机变量$R_t$和$S_t$具有定义明确的离散概率分布，并且仅依赖于前继动作。也就是说给定前继状态和动作的值时，这些随机变量的特定值，$s^{'}\in S和r\in R$，在t时刻出现的概率是：

$$
p(s^{'},r|s,a) \dot= Pr\{S_t=s^{'},R_t=r|S_{t-1}=s,A_{t-1}=a\}
$$
函数p定义了MDP的动态特性。由p给出的概率完全刻画了环境的动态特性，也就是说$S_t$和$R_t$的每个可能的值出现的概率只取决于前一个状态$S_{t-1}$和前一个动作$A_{t-1}$，并且与更早之前的状态和动作完全无关。 我们认为这样的状态是具**马尔科夫性**的。
除去p式外，还有：
$$
p(s^{'}|s,a) = Pr\{S_t = s^{'}|S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in R}p(s^{'},r|s,a).
$$
$$
r(s,a) \dot= \mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a] = \sum_{r\in R}r \sum_{s^{'}\in S}p(s^{'},r|s,a)
$$
$$
r(s,a,s^{'}) \dot= \mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s^{'}] = \sum_{r\in R}r\frac{p(s^{'},r|s,a)}{p(s^{'}|s,a)}
$$

我们遵循的一般规则是，智能体不能改变的事务都被认为是在外部的，即是环境的一部分。也就是说，智能体和环境的界限划分仅仅决定了智能体进行绝对控制的边界，而不是其知识的边界。
任何目标导向的行为的学习问题都可以概括为智能体及其环境之间来回传递的三个信号：一个信号用来表示智能体做出的选择（行动），一个信号用来表示做出该选择的基础（状态），还有一个信号用来定义智能体的目标（收益）。**性能极易受到其表征方式的影响。**

## 3.2 目标和收益

智能体的**目标**被形式化表征为一种特殊信号，称为**收益**。 在每个时刻，收益都是单一标量数值，$R_t \in \mathbb{R}$。**目标：最大化智能体接收到的标量信号（称之为收益）累积和的概率期望值。** 即，我们设立收益的方式要能真正表明我们的目标。但是，收益信号不是传授智能体*如何实现目标的先验知识。* 收益信号智能用来传达什么是你想要实现的目标 ，而不是如何实现这个目标。

## 3.3 回报和分幕

如果把t时刻后接收的收益序列表示为$R_{t+1},R_{t+2},\dotsb$，那么我们寻求的是最大化的期望回报，记为$G_t$，
$$
G_t \dot= R_{t+1}+R_{t+2}+R_{t+3}+\dotsb+R_{T},
$$
其中$T$是最终时刻。智能体和环境的交互能被自然地分成一系列子序列（每个序列都存在最终时刻），我们称每个子序列为幕(episodes)。而对于持续性任务，即$T=\infty$，那么试图最大化的回报也很容易趋于无穷。为此，我们引入**折扣** 。根据这种方法，智能体尝试选择动作，使得它在未来收到的经过折扣系数加权后的收益总和是最大化的。
$$
G_t \dot= R_{t+1}+ \gamma R_{t+2} + \gamma^2R_{t+3}+\dotsb = \sum_{k=0}^\infty \gamma^kR_{t+k+1}
$$
我们可以通过递推公式得：
$$
G_t \dot= R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \dotsb
= R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3} + \dotsb)
= R_{t+1} + \gamma G_{t+1}
$$

## 3.4 分幕式和持续性任务的统一表示法

$$
G_t \dot= \sum^{T}_{k=t+1}\gamma^{k-t-1}R_k
$$

## 3.5 策略和价值函数

价值函数是状态（或状态与动作二元组）的函数，用来评估当前智能体在给定状态（或给定状态与动作）下有多好。价值函数是与特定的行为方式相关的，我们称之为策略。严格说，策略是从状态到每个动作的选择概率之间的映射。

价值函数$v_\pi$和$q_\pi$都能从经验中估算得到。比如，如果一个智能体遵循策略$\pi$，并且对每个遇到的状态都记录该状态后的实际回报的平均值，那么随着状态出现的次数接近无穷大，这个平均值会收敛到状态价值$v_\pi(s)$，如果为每个状态的每个动作都保留单独的平均值，那么类似的，这些平均值也会收敛到动作价值$q_\pi(s,a)$。我们将这种估算的方法称之为**蒙特卡洛法**。当状态量很大时，对每一个状态独立估计是不现实的，所以我们对价值函数参数化，通过调整价值函数的参数来更好地计算回报值。

在强化学习和动态规划中，价值函数有一个基本特性，即满足某种递归关系。
$$
v_\pi(s) \dot= \mathbb{E}_\pi[G_t|S_t=s] \\
= \mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s] \\
= \sum_a\pi(a|s)\sum_{s'}\sum_{r}p(s',r|s,a)[r+\gamma\mathbb{E}[G_{t+1}|S_{t+1}=s']] \\
= \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]
$$
上式被称为价值函数的贝尔曼方程。

## 3.6 最优策略和最优价值函数

解决一个强化学习任务就意味着要找出一个策略，使其能够在长期过程中获得大量收益。在本质上，价值函数定义了策略上的一个偏序关系。即如果说以一个策略$\pi$和另一个策略$\pi'$相差不多或者更好，那么其所有状态上的期望回报都应该等于或大于$\pi'$的期望回报。总存在一个策略不劣于其他所有策略，这就是最优策略，称之为$\pi_*$，最优状态价值函数$v_*$。
$$
v_*(s) \dot= {max}_\pi v_\pi(s)
$$
最优的策略也共享最优动作价值函数，记为$q_*$。
$$
q_*(s,a) = \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a]
$$

贝尔曼最优方程阐述了一个事实：最优策略下各个状态的价值一定等于这个状态下最有动作的期望回报。
$$
v_*(s) = max_{a\in A(s)}q_{\pi_*}(s,a) \\
= max_a\mathbb{E}_{\pi_*}[G_t|S_t=s,A_t=a] \\
= max_a\mathbb{E}_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a] \\
= max_a\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a] \\
= max_a\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]
$$

$$
q_*(s,a) = \mathbb{E}[R_{t+1}+\gamma max_{a'}q_*(S_{t+1},a')|S_t=s,A_t=a] \\
= \sum_{s',r}p(s',r|s,a)[r+\gamma max_{a'}q_*(s',a')]
$$

一旦有$v_*$，单步搜索最好的动作就是全局最优动作。即贪心策略是最优策略。$v_*$最优意思的在于即使是单步的策略，在长远来看也是最优的，原因是$v_*$已经包含了未来所有可能的行为所产生的回报影响。那么定义$v_*$的意义就在于，我们可以将最优的长期回报期望转化为每个状态对应的一个当前局部量的计算。

## 3.7 最优性和近似算法

存储量是一个比较大的约束。在状态集合小而有限的任务中，用数组或者表格来估计每个状态（或”状态-动作“二元组）是有可能的。我们称之为表格型任务，对应的方法我们称之为表格型方法。但在许多实际情况下，经常有很多状态是不能用表格中的一行来表示的，在这些情况下，价值函数必须用近似算法，这时通常使用参数化函数表示方法。

强化学习在线运行的本质使得它在近似最优策略的过程中，对于那些经常出现的状态集合会花更多的努力去学习好的决策，而代价就是对不经常出现的状态给予的学习力度不够。这是区分强化学习和其他解决MDP问题的近似方法的重要判断依据。

## 习题

### 练习 3.1

1. 无人驾驶。 动作：汽车方向盘转角、油门、刹车。 观察：GPS导航得知的与目标点的距离，雷达感知范围内与障碍物的距离，摄像头感知到的交通信号灯与车道等信息。 奖励：朝着目标行驶，没有交通违规行为，没有发生交通事故。
2. 机械臂抓取。 动作：机器臂舵机信号电量。 观察：机械臂当前的姿态，以及目标的位置。 奖励 ：抓取到相关物体。
3. 金融市场。 动作：买进卖出股票种类与股票量。 观察：该股票的历史走势以及近期相关信息。 奖励：金钱收益。

### 练习 3.2

不能。 MDP只能表述具有马尔科夫性质的目标导向的学习任务。而一些隐马尔科夫过程，比如说扑克游戏，只能观测到牌桌上已经打出的牌，无法观测到其他玩家的手牌，所以无法直接用MDP。

### 练习 3.3

智能体和环境的划分取决于任务的选定。
如果是控制车，那么控制的就是油门、方向盘和刹车。
如果考虑的智能体是人，就考虑的是肌肉的收缩。
如果考虑的是任务规划，就考虑的是目的地的选择。

### 练习 3.4

s | a | $s^{'}$ | r | p(s',r\s,a)
:-: | :-: | :-: | :-: | :-:
high | search | high | $r_{search}$ | $\alpha$
high | search| low | $r_{search}$ | $1-\alpha$
high | wait| high | $r_{wait}$ | $1$
high | wait| low | $r_{wait}$ | $0$
low | search| high | $-3$ | $1-\beta$
low | search| low | $r_{search}$ | $\beta$
low | wait| high | $r_{wait}$ | $0$
low | wait| low | $r_{wait}$ | $1$
low | recharge| high | $0$ | $1$
low | recharge| low | $0$ | $0$

### 练习 3.5

由
$$
\sum_{s'\in S}\sum_{r\in R}p(s',r|s,a) = 1
$$
修改为
$$
\sum_{s'\in S'}\sum_{r\in R}p(s',r|s,a) = 1
$$

### 练习 3.6

对于分幕式任务来说：
$G_t = -\gamma^{T-t}$
对于持续性来说：
$G_t = -\sum_{k\in \mathcal{K}}^{\infty}\gamma$^{k-t}
k是在t时刻后所有的失败的时刻集合。

### 练习 3.7

如果说对于一个连续任务来说，在无穷时刻处，随机探索的机器人也可以走出迷宫，所以在对其收益应该加入惩罚，即每走一步都加入一个负的收益。

### 练习 3.8

$G_5 = 0$
$G_4 = R_5 + \gamma G_5 = 2 + 0 = 2$
$G_3 = R_4 + \gamma G_4 = 3 + 0.5\times 2 = 4$
$G_2 = R_3 + \gamma G_3 = 6 + 0.5\times 4 = 8$
$G_1 = R_2 + \gamma G_2 = 2 + 0.5\times 8 = 6$
$G_0 = R_1 + \gamma G_1 = 1 + 0.5\times 6 = 4$

### 练习 3.9

$G_0 = 2 + \gamma \times 7 + \gamma ^2 \times 7 + \dotsb = 2 + \sum_{k=1}^{\infty}7·\gamma^k = 2 + 7 \frac{\gamma}{1-\gamma}$

$G_0 = 7 \frac{\gamma}{1-\gamma}$

### 练习 3.10

证明：
$$
G_t = \sum_{k=0}^{\infty}\gamma^{k} = 1 + \gamma + \gamma^2 + \gamma^3 + \dotsb
$$

$$
\gamma G_t = \gamma + \gamma ^2 + \dotsb = G_t - 1
$$

$$
(1-\gamma G_t) = 1
$$

$$
G_t = \frac{1}{1-\gamma}
$$

### 练习 3.11

$$
\mathbb{E}_{\pi}[R_{t+1}|S_t=s] = \sum_a\pi(a|s)\sum_{s',r}rp(s',r|s,a)
$$

### 练习 3.12

$$
v_{\pi}(s) = \sum_a \pi(a|s)·q_{\pi}(s,a)
$$

### 练习 3.13

$$
q_\pi \dot= \mathbb{E}_\pi[G_t|S_t=s,A_t=a] = \sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]
$$

### 练习 3.14

在例3.5中，我们可以根据题意知，当前后左右是2.3，-0.4，0.7和0.4时，假设动作的收益为r1,r2,r3,r4，那么其中间收益为：
1/4(r1+r2+r3+r4)+1/4*2.7 = 1/4(r1+r2+r3+r4)+0.7
那么该问题就为证明r1,r2,r3,r4为0。由于其前后左右有价值函数，所以没有出界，即r不为-1。
其次，若r1,r2,r3,r4中有奖励r为+10或+5，该位置的价值函数应大于10-2*0.9 = 8.2或5-2*0.9=3.2。故而收益都为0，该点位+0.7。

### 练习 3.15

$$
\because
G_t = R_{t+1} + \gamma R_{t+2} + \gamma ^2 R_{t+3} + \dotsb\\
G'_t = R_{t+1}+c + \gamma (R_{t+2}+c) + \dotsb \\
\therefore G'_t = G_t + c\sum_{k=0}^{\infin}\gamma^k \\
G'_t = G_t + c \frac{1}{1-\gamma}
$$

### 练习 3.16

当任务是分幕式时，会产生影响。
假设结束的时刻为$T$，折扣率为$\gamma$，加入的额外收益为$c$。
将练习3.15的公式改写为：
$$
G'_t = G_t + c\sum_{k=0}^{T-t-1}\gamma^k \\
G'_t = G_t + c\frac{1-\gamma^{T-t}}{1-\gamma}
$$
在这种情况下，$v_c$就不是一个常数，所以会产生影响。

### 练习 3.17

$$
q_\pi(s,a) = \sum_{s',r} p(s',r|s,a) [r + \gamma \sum_{a'} \pi(a'|s') q_\pi(s',a')]
$$

### 练习 3.18

$$
v_\pi(s) = \sum_a \pi(a|s)q_\pi(s,a)
$$

### 练习 3.19

$$
q_\pi(s,a) = \sum_{s',r}p(s',r|s,a)[r+v_\pi(s')]
$$  

### 练习 3.20

最优状态价值即推杆和木杆的组合，在绿色场地之外时使用木杆，在绿色场地上使用推杆。

### 练习 3.21

对于推杆的最优动作价值函数与对于推杆的最优价值函数一样。

### 练习 3.22

由
$$
v_\pi(s) = \mathbb{E}_\pi(G_t|S_t=s) = \mathbb{E}_\pi[\sum_{k=0}^{\infty}\gamma^k R_{k+1}|S_t=t]
$$
得：
当$\gamma=0$时，$v_{\pi_{left}}(s) = 1$，而$v_{\pi_{right}}(s) = 0$，所以$\pi_{left}$是最优的。

当$\gamma = 0.5$时，$v_{\pi_{left}}(s) = 1+0.5^2+0.5^4+\dotsb$
而$v_{\pi_{right}}(s) = 2*0.5+2*0.5^3+2*0.5^5+\dotsb = 1 + 0.5^2 + 0.5^4 + \dotsb$
所以当$\gamma=0.5$时，$\pi_{left}$和$\pi_{right}$都是最优的。

当$\gamma = 0.9$时，$v_{\pi_{left}}(s) = 1+0.9^2+0.9^4+\dotsb$
而$v_{\pi_{right}}(s) = 2\times0.9+2\times0.9^3+2\times0.9^5+\dotsb = 1.8 + 1.8\times0.9^2 + 1.8\times0.9^4 + \dotsb = 1.8v_{\pi_{left}}(s)$
所以当$\gamma=0.9$时，$\pi_{right}$是最优的。

## 练习 3.23

$$
q_*(s,a) = \sum_{s',r}p(s',r|s,a)[r + \gamma max_{a'}q_*(s',a')]
$$

## 练习 3.24

$$
v = 10+0.9\times16 = 24.4
$$

## 练习 3.25

$$
v_*(s) = \sum_{a} \pi_*(a|s) q_*(s,a)
$$

## 练习 3.26

$$
q_*(s,a) = \sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]
$$

## 练习 3.27

$$
\pi_*(a|s) = \frac{\mathbb{I(a = max_{a}q_*(s,a))}}{\sum_a\mathbb{I}(a = max_{a}q_*(s,a))}
$$

## 练习 3.28

$$
\pi_*(a|s) = \frac{\mathbb{I}(a = max_{a}\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')])}{\sum_a\mathbb{I}(a = max_{a}\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]}
$$

## 练习 3.29

关于 $v_\pi(s)$的贝尔曼方程：
$$
v_\pi(s)=\sum_a \pi(a|s) q_\pi(s,a) \\
= \sum_a \pi(a|s)[r(s,a) + \gamma \sum_{s'}p(s'|s,a)v_\pi(s')]
$$

关于 $v_*(s)$的贝尔曼方程：
$$
v_*(s)= max_a[r(s,a) + \gamma \sum_{s'}p(s'|s,a)v_*(s')]
$$

关于 $q_\pi(s,a)$的贝尔曼方程：
$$
q_\pi(s,a) = r(s,a) + \gamma \sum_{s'} p(s'|s,a)v_\pi(s') \\
= r(s,a) + \gamma \sum_{s'}p(s'|s,a)[\sum_{a'}\pi(a'|s')q_\pi(s',a')]
$$

关于 $q_*(s,a)$的贝尔曼方程：

$$
q_*(s,a) = r(s,a) + \gamma \sum_{s'}p(s'|s,a) max_{a'}q_*(s',a')
$$